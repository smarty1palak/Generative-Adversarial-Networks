{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TripleGAN.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kpnMqLMjsZXI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": []
        },
        "outputId": "41246d98-f289-4e90-d7eb-d7ea869e4ace",
        "extraOutputs": [
          {
            "data": [],
            "output_extras": [],
            "id": "4f24b0be-84fd-428f-9c17-85696b213b95"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "b8221242-a9d4-44f8-d076-4f251f3d5007"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "a10352e9-bb3d-49f1-ed4e-c1c6c6483b7e"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "d368a298-6f77-4089-a395-f696c0696d21"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "873ea3b1-9deb-4732-859a-249ce49b4e9f"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "a690a1a3-847f-4f49-9042-e67d64b23985"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "e1d4d9ac-b2a0-4f49-ac36-411e4d3008b6"
          },
          {
            "data": [
              {
                "output_type": "stream",
                "text": [
                  "gpg: keybox '/tmp/tmpyht0hukq/pubring.gpg' created\n",
                  "gpg: /tmp/tmpyht0hukq/trustdb.gpg: trustdb created\n",
                  "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
                  "gpg: Total number processed: 1\n",
                  "gpg:               imported: 1\n",
                  "Warning: apt-key output should not be parsed (stdout is not a terminal)\n"
                ],
                "name": "stdout"
              }
            ],
            "output_extras": [
              {
                "item_id": 4
              }
            ],
            "id": "fa6a4f2b-2935-47b5-8d06-fafac24e2587",
            "info": {
              "status": "ok",
              "timestamp": 1521120520936,
              "user_tz": -330,
              "elapsed": 10739,
              "user": {
                "displayName": "Mahir Jain",
                "photoUrl": "//lh5.googleusercontent.com/-_7h35iDcVws/AAAAAAAAAAI/AAAAAAAAGgM/vCXCOFiWKJs/s50-c-k-no/photo.jpg",
                "userId": "102382435924443615177"
              },
              "execution_count": 1
            },
            "base_uri": "https://localhost:8080/",
            "height": 119
          }
        ]
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_cRuL80tCIs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZgwjpyZto5a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": []
        },
        "outputId": "32fa3dc3-d21b-42dc-a618-637342b87d37",
        "extraOutputs": [
          {
            "data": [],
            "output_extras": [],
            "id": "05cc6b2f-a5c2-4955-9406-5da4e99c6067"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "1d03a363-6c4d-4eca-f225-71e891dfe094"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "be137629-57c0-4fca-d58c-baffa82b9e1b"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "e0aa1dc5-73c3-4400-e43e-b809904db8f2"
          },
          {
            "data": [
              {
                "output_type": "error",
                "ename": "KeyboardInterrupt",
                "evalue": "ignored",
                "traceback": [
                  "\u001b[0;31m\u001b[0m",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
                  "\u001b[0;32m<ipython-input-3-3d0ee3556bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                ]
              }
            ],
            "output_extras": [
              {
                "item_id": 1
              }
            ],
            "id": "3a285ae2-8462-4e35-f4ad-75ac51cbe10c",
            "info": {
              "status": "error",
              "timestamp": 1521111313101,
              "user_tz": -330,
              "elapsed": 12444,
              "user": {
                "displayName": "Mahir Jain",
                "photoUrl": "//lh5.googleusercontent.com/-_7h35iDcVws/AAAAAAAAAAI/AAAAAAAAGgM/vCXCOFiWKJs/s50-c-k-no/photo.jpg",
                "userId": "102382435924443615177"
              },
              "execution_count": 3
            },
            "base_uri": "https://localhost:8080/",
            "height": 452
          }
        ]
      },
      "cell_type": "code",
      "source": [
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEm5hcG1uBDM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": []
        },
        "outputId": "d29286e7-f700-4136-a236-200851a69f7a",
        "extraOutputs": [
          {
            "data": [],
            "output_extras": [],
            "id": "bb9ae67f-0d71-4775-e83c-873a18179d10"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "714dcd49-0667-40dc-8bf1-f5c320765438"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "6511491c-b0f9-473e-84cb-2c987b018f26"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "7d7b9cd8-1153-4214-f05b-3c01d6c2d71a"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "9779848e-1549-4bcc-9d2d-582eb4988fba"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "66b4b439-fe0e-4b77-86db-892737db47df"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "1004dc68-4b93-4022-a11c-b3f0a480fdc3"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "ad86e68a-344a-4b07-d88a-9add3c198c6b"
          },
          {
            "data": [
              {
                "output_type": "stream",
                "text": [
                  "fuse: mountpoint is not empty\r\n",
                  "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
                  "16CO109-123-135-P5\n",
                  "16CO123-146-Asn1.pdf\n",
                  "2017-18-MI-B1-Q.txt.odt\n",
                  "2017-18-MI-B1-Q.txt.odt (7c8d1e8a)\n",
                  "31st slots.ods.ods\n",
                  "31st slots.ods.ods (2dc8e449)\n",
                  "31st slots.ods.ods (99d1f79e)\n",
                  "31st slots.ods.ods (b5bd4839)\n",
                  "Abstract-DDS.odt\n",
                  "asn1_sw_lab.odt\n",
                  "ASSIGNMENT-1.odt\n",
                  "assignment-1Qns.PDF\n",
                  "Classroom\n",
                  "CLI.pptx\n",
                  "Cloud talk.odt\n",
                  "CO200-Insem.ods.ods\n",
                  "CO200-Insem.ods.ods (3d6ce5b4)\n",
                  "Colab Notebooks\n",
                  "content:__com.google.android.apps.gsa.velvet.extradex.ExtraDexHostProvider_assist.com.google.android.apps.gsa.assist.ScreenshotProvider_com.google.android.apps.gsa.assist.ScreenshotProvider_ScreenAssistScreenshots_Screenshot_20160923-015714.png\n",
                  "Copy of Slam Dunk'17 T-Shirt Order.zip\n",
                  "Decipher (Responses).ods\n",
                  "Decipher.zip\n",
                  "Guru_SMP_Report.txt.odt\n",
                  "IC.odp.pdf (77ea98e7)\n",
                  "Jersey Design\n",
                  "Khoj.xls\n",
                  "Khoj.xls.ods\n",
                  "NITK Surathkal Mahir Jain\n",
                  "operating systems lab course   plan.odt\n",
                  "Photo from Mahir Jain\n",
                  "Photo from Mahir Jain (8ec952fc)\n",
                  "Report1-123-146.pdf\n",
                  "RE-requirementEngg.ppt.pdf\n",
                  "Screenshot_20170530-170300.png\n",
                  "SE-a1-2.docx\n",
                  "SE-a1-2.docx.odt\n",
                  "SE-a1 (4346b982).pptx\n",
                  "SE-a1.pptx\n",
                  "SE-a1.pptx.pdf\n",
                  "SE Report-II.odt\n",
                  "Slam Dunk'17 T-Shirt Order (Responses).ods\n",
                  "Slam Dunk'17 T-Shirt Order.zip\n",
                  "SRS.odt\n",
                  "SW Engg Jan-May2018\n",
                  "T shirt confirmation.zip\n",
                  "UML-Documentation.odt\n",
                  "Untitled\n",
                  "Untitled document.odt\n",
                  "Versus (Responses).ods\n",
                  "Versus.zip\n"
                ],
                "name": "stdout"
              }
            ],
            "output_extras": [
              {
                "item_id": 2
              }
            ],
            "id": "a907ce4a-e4ef-462f-f8ae-e53adc4e894b",
            "info": {
              "status": "ok",
              "timestamp": 1521120553362,
              "user_tz": -330,
              "elapsed": 1971,
              "user": {
                "displayName": "Mahir Jain",
                "photoUrl": "//lh5.googleusercontent.com/-_7h35iDcVws/AAAAAAAAAAI/AAAAAAAAGgM/vCXCOFiWKJs/s50-c-k-no/photo.jpg",
                "userId": "102382435924443615177"
              },
              "execution_count": 3
            },
            "base_uri": "https://localhost:8080/",
            "height": 921
          }
        ]
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "!ls drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0X-8hbvy9RMv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": []
        },
        "outputId": "47e9d5d3-94bd-401e-f62f-7e142d3d982f",
        "extraOutputs": [
          {
            "data": [],
            "output_extras": [],
            "id": "a307d063-2cbb-4479-8fb1-01c0a93b99bb"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "8c72f869-3689-4d16-ef24-dcb0f0f9761d"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "1fce91c2-9df7-442e-f620-77b5fcb2e472"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "8d5d18e9-c040-47c2-bc73-1025a516199a"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "5ed773dc-9deb-457e-eb0d-c23edf0418f6"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "80626758-7002-41a8-f232-11df233e1b9c"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "5cc77598-d51f-447f-dd0e-00b4207ab84f"
          },
          {
            "data": [
              {
                "output_type": "stream",
                "text": [
                  "Requirement already satisfied: tflearn in /usr/local/lib/python2.7/dist-packages\r\n",
                  "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from tflearn)\r\n",
                  "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from tflearn)\r\n",
                  "Requirement already satisfied: Pillow in /usr/local/lib/python2.7/dist-packages (from tflearn)\r\n",
                  "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow->tflearn)\r\n"
                ],
                "name": "stdout"
              }
            ],
            "output_extras": [
              {
                "item_id": 1
              }
            ],
            "id": "5afeb6fd-404b-468e-82e6-d44488b9bb9a",
            "info": {
              "status": "ok",
              "timestamp": 1521120561888,
              "user_tz": -330,
              "elapsed": 2085,
              "user": {
                "displayName": "Mahir Jain",
                "photoUrl": "//lh5.googleusercontent.com/-_7h35iDcVws/AAAAAAAAAAI/AAAAAAAAGgM/vCXCOFiWKJs/s50-c-k-no/photo.jpg",
                "userId": "102382435924443615177"
              },
              "execution_count": 4
            },
            "base_uri": "https://localhost:8080/",
            "height": 102
          }
        ]
      },
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KtOFtYFi7Cdy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": []
        },
        "outputId": "44bdfebb-719b-4072-be02-867547c10b59",
        "extraOutputs": [
          {
            "data": [],
            "output_extras": [],
            "id": "c8666530-825f-4413-d3c0-1759bdc2739d"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "0e31a771-599a-419b-cc74-ab2dfb55284a"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "2cd9575a-f5ad-459b-f6f2-4519a6a0b816"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "93d8e22b-4990-434e-ff9b-5e4f7f58549b"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "4bb100ff-d10c-4a61-dadc-acf9a8cf7107"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "8798e07c-bbd9-49fd-dbbd-533c9f43638f"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "17bfc407-ba3f-40b8-9722-899ebbafba13"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "45a73d65-9fdb-4dad-9270-6bd437a72c49"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "d4c50373-0858-46fb-9a84-1cba166a53c2"
          },
          {
            "data": [
              {
                "output_type": "stream",
                "text": [
                  "Using TensorFlow backend.\n"
                ],
                "name": "stderr"
              }
            ],
            "output_extras": [
              {
                "item_id": 1
              }
            ],
            "id": "fad90016-3b2c-4578-f3c2-60dd5787f025",
            "info": {
              "status": "ok",
              "timestamp": 1521120563586,
              "user_tz": -330,
              "elapsed": 1628,
              "user": {
                "displayName": "Mahir Jain",
                "photoUrl": "//lh5.googleusercontent.com/-_7h35iDcVws/AAAAAAAAAAI/AAAAAAAAGgM/vCXCOFiWKJs/s50-c-k-no/photo.jpg",
                "userId": "102382435924443615177"
              },
              "execution_count": 5
            },
            "base_uri": "https://localhost:8080/",
            "height": 34
          }
        ]
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tflearn import global_avg_pool\n",
        "from tensorflow.contrib.layers import variance_scaling_initializer\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from keras.datasets import cifar10\n",
        "import time\n",
        "import tensorflow.contrib.slim as slim\n",
        "import scipy.misc, os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CrNZ3woM-edH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Utils.py\n",
        "\n",
        "def show_all_variables():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "\n",
        "def check_folder(log_dir):\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def plot(samples):\n",
        "    fig  = plt.figure(figsize = (4,4))\n",
        "    gs = gridspec.GridSpec(4,4)\n",
        "    gs.update(wspace = 0.05,hspace=0.05)\n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        plt.imshow(sample)\n",
        "\n",
        "    return fig\n",
        "    \n",
        "def save_images(images, size, image_path):\n",
        "    return imsave(inverse_transform(images), size, image_path)\n",
        "\n",
        "def imsave(images, size, path):\n",
        "    image = np.squeeze(merge(images, size))\n",
        "    plt.imshow(image)\n",
        "    return scipy.misc.imsave(path, image)\n",
        "\n",
        "def inverse_transform(images):\n",
        "    return (images+1.)/2.\n",
        "    # return ((images + 1.) * 127.5).astype('uint8')\n",
        "\n",
        "def merge(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    if (images.shape[3] in (3,4)):\n",
        "        c = images.shape[3]\n",
        "        img = np.zeros((h * size[0], w * size[1], c))\n",
        "        for idx, image in enumerate(images):\n",
        "            i = idx % size[1]\n",
        "            j = idx // size[1]\n",
        "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "        return img\n",
        "    elif images.shape[3]==1:\n",
        "        img = np.zeros((h * size[0], w * size[1]))\n",
        "        for idx, image in enumerate(images):\n",
        "            i = idx % size[1]\n",
        "            j = idx // size[1]\n",
        "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
        "        return img\n",
        "    else:\n",
        "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tOBmMo3n-uxR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ops.py\n",
        "\n",
        "\n",
        "he_init = variance_scaling_initializer()\n",
        "# he_init = tf.truncated_normal_initializer(stddev=0.02)\n",
        "\"\"\"\n",
        "The weight norm is not implemented at this time.\n",
        "\"\"\"\n",
        "\n",
        "def weight_norm(x, output_dim) :\n",
        "    input_dim = int(x.get_shape()[-1])\n",
        "    g = tf.get_variable('g_scalar', shape=[output_dim], dtype=tf.float32, initializer=tf.ones_initializer())\n",
        "    w = tf.get_variable('weight', shape=[input_dim, output_dim], dtype=tf.float32, initializer=he_init)\n",
        "    w_init = tf.nn.l2_normalize(w, dim=0) * g  # SAME dim=1\n",
        "\n",
        "    return tf.variables_initializer(w_init)\n",
        "\n",
        "def conv_layer(x, filter_size, kernel, stride=1, padding='SAME', wn=False, layer_name=\"conv\"):\n",
        "    with tf.name_scope(layer_name):\n",
        "        if wn:\n",
        "            w_init = weight_norm(x, filter_size)\n",
        "\n",
        "            x = tf.layers.conv2d(inputs=x, filters=filter_size, kernel_size=kernel, kernel_initializer=w_init, strides=stride, padding=padding)\n",
        "        else :\n",
        "            x = tf.layers.conv2d(inputs=x, filters=filter_size, kernel_size=kernel, kernel_initializer=he_init, strides=stride, padding=padding)\n",
        "        return x\n",
        "\n",
        "\n",
        "def deconv_layer(x, filter_size, kernel, stride=1, padding='SAME', wn=False, layer_name='deconv'):\n",
        "    with tf.name_scope(layer_name):\n",
        "        if wn :\n",
        "            w_init = weight_norm(x, filter_size)\n",
        "            x = tf.layers.conv2d_transpose(inputs=x, filters=filter_size, kernel_size=kernel, kernel_initializer=w_init, strides=stride, padding=padding)\n",
        "        else :\n",
        "            x = tf.layers.conv2d_transpose(inputs=x, filters=filter_size, kernel_size=kernel, kernel_initializer=he_init, strides=stride, padding=padding)\n",
        "        return x\n",
        "\n",
        "\n",
        "def linear(x, unit, wn=False, layer_name='linear'):\n",
        "    with tf.name_scope(layer_name):\n",
        "        if wn :\n",
        "            w_init = weight_norm(x, unit)\n",
        "            x = tf.layers.dense(inputs=x, units=unit, kernel_initializer=w_init)\n",
        "        else :\n",
        "            x = tf.layers.dense(inputs=x, units=unit, kernel_initializer=he_init)\n",
        "        return x\n",
        "\n",
        "\n",
        "def nin(x, unit, wn=False, layer_name='nin'):\n",
        "    # https://github.com/openai/weightnorm/blob/master/tensorflow/nn.py\n",
        "    with tf.name_scope(layer_name):\n",
        "        s = list(map(int, x.get_shape()))\n",
        "        x = tf.reshape(x, [np.prod(s[:-1]), s[-1]])\n",
        "        x = linear(x, unit, wn, layer_name)\n",
        "        x = tf.reshape(x, s[:-1] + [unit])\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def gaussian_noise_layer(x, std=0.15):\n",
        "    noise = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=std, dtype=tf.float32)\n",
        "    return x + noise\n",
        "\n",
        "def Global_Average_Pooling(x):\n",
        "    return global_avg_pool(x, name='Global_avg_pooling')\n",
        "\n",
        "\n",
        "def max_pooling(x, kernel, stride):\n",
        "    return tf.layers.max_pooling2d(x, pool_size=kernel, strides=stride, padding='VALID')\n",
        "\n",
        "\n",
        "def flatten(x):\n",
        "    return tf.contrib.layers.flatten(x)\n",
        "\n",
        "\n",
        "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
        "    return tf.maximum(x, leak * x)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return tf.nn.sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return tf.nn.tanh(x)\n",
        "\n",
        "def conv_concat(x, y):\n",
        "    x_shapes = x.get_shape()\n",
        "    y_shapes = y.get_shape()\n",
        "\n",
        "    return concat([x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], axis=3)\n",
        "\n",
        "\n",
        "def concat(x, axis=1):\n",
        "    return tf.concat(x, axis=axis)\n",
        "\n",
        "\n",
        "def reshape(x, shape):\n",
        "    return tf.reshape(x, shape=shape)\n",
        "\n",
        "\n",
        "def batch_norm(x, is_training, scope):\n",
        "    return tf.contrib.layers.batch_norm(x,\n",
        "                                        decay=0.9,\n",
        "                                        updates_collections=None,\n",
        "                                        epsilon=1e-5,\n",
        "                                        scale=True,\n",
        "                                        is_training=is_training,\n",
        "                                        scope=scope)\n",
        "\n",
        "def instance_norm(x, is_training, scope):\n",
        "    with tf.variable_scope(scope):\n",
        "        epsilon = 1e-5\n",
        "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
        "        scale = tf.get_variable('scale', [x.get_shape()[-1]],\n",
        "                                initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n",
        "        offset = tf.get_variable('offset', [x.get_shape()[-1]], initializer=tf.constant_initializer(0.0))\n",
        "        out = scale * tf.div(x - mean, tf.sqrt(var + epsilon)) + offset\n",
        "\n",
        "        return out\n",
        "\n",
        "def dropout(x, rate, is_training):\n",
        "    return tf.layers.dropout(inputs=x, rate=rate, training=is_training)\n",
        "\n",
        "def rampup(epoch):\n",
        "    if epoch < 80:\n",
        "        p = max(0.0, float(epoch)) / float(80)\n",
        "        p = 1.0 - p\n",
        "        return math.exp(-p*p*5.0)\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "def rampdown(epoch):\n",
        "    if epoch >= (300 - 50):\n",
        "        ep = (epoch - (300 - 50)) * 0.5\n",
        "        return math.exp(-(ep * ep) / 50)\n",
        "    else:\n",
        "        return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8C6f09A-7OO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# cifar10.py\n",
        "\n",
        "\n",
        "class_num = 10\n",
        "image_size = 32\n",
        "img_channels = 3\n",
        "\n",
        "\n",
        "def prepare_data(n):\n",
        "    (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
        "    train_data, test_data = color_preprocessing(train_data, test_data) # pre-processing\n",
        "\n",
        "    criteria = n//10\n",
        "    input_dict, labelled_x, labelled_y, unlabelled_x, unlabelled_y = defaultdict(int), list(), list(), list(), list()\n",
        "\n",
        "    for image, label in zip(train_data,train_labels) :\n",
        "        if input_dict[int(label)] != criteria :\n",
        "            input_dict[int(label)] += 1\n",
        "            labelled_x.append(image)\n",
        "            labelled_y.append(label)\n",
        "\n",
        "        unlabelled_x.append(image)\n",
        "        unlabelled_y.append(label)\n",
        "\n",
        "\n",
        "    labelled_x = np.asarray(labelled_x)\n",
        "    labelled_y = np.asarray(labelled_y)\n",
        "    unlabelled_x = np.asarray(unlabelled_x)\n",
        "    unlabelled_y = np.asarray(unlabelled_y)\n",
        "\n",
        "    print(\"labelled data:\", np.shape(labelled_x), np.shape(labelled_y))\n",
        "    print(\"unlabelled data :\", np.shape(unlabelled_x), np.shape(unlabelled_y))\n",
        "    print(\"Test data :\", np.shape(test_data), np.shape(test_labels))\n",
        "    print(\"======Load finished======\")\n",
        "\n",
        "    print(\"======Shuffling data======\")\n",
        "    indices = np.random.permutation(len(labelled_x))\n",
        "    labelled_x = labelled_x[indices]\n",
        "    labelled_y = labelled_y[indices]\n",
        "\n",
        "    indices = np.random.permutation(len(unlabelled_x))\n",
        "    unlabelled_x = unlabelled_x[indices]\n",
        "    unlabelled_y = unlabelled_y[indices]\n",
        "\n",
        "    print(\"======Prepare Finished======\")\n",
        "\n",
        "\n",
        "    labelled_y_vec = np.zeros((len(labelled_y), 10), dtype=np.float)\n",
        "    for i, label in enumerate(labelled_y) :\n",
        "        labelled_y_vec[i, labelled_y[i]] = 1.0\n",
        "\n",
        "    unlabelled_y_vec = np.zeros((len(unlabelled_y), 10), dtype=np.float)\n",
        "    for i, label in enumerate(unlabelled_y) :\n",
        "        unlabelled_y_vec[i, unlabelled_y[i]] = 1.0\n",
        "\n",
        "    test_labels_vec = np.zeros((len(test_labels), 10), dtype=np.float)\n",
        "    for i, label in enumerate(test_labels) :\n",
        "        test_labels_vec[i, test_labels[i]] = 1.0\n",
        "\n",
        "\n",
        "    return labelled_x, labelled_y_vec, unlabelled_x, unlabelled_y_vec, test_data, test_labels_vec\n",
        "\n",
        "\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "    oshape = np.shape(batch[0])\n",
        "\n",
        "    if padding:\n",
        "        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
        "    new_batch = []\n",
        "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "    for i in range(len(batch)):\n",
        "        new_batch.append(batch[i])\n",
        "        if padding:\n",
        "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                      mode='constant', constant_values=0)\n",
        "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                       nw:nw + crop_shape[1]]\n",
        "    return new_batch\n",
        "\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "    for i in range(len(batch)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            batch[i] = np.fliplr(batch[i])\n",
        "    return batch\n",
        "\n",
        "\n",
        "def color_preprocessing(x_train, x_test):\n",
        "    x_train = x_train/127.5 - 1\n",
        "    x_test = x_test/127.5 - 1\n",
        "    return x_train, x_test\n",
        "\n",
        "\n",
        "def data_augmentation(batch):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    batch = _random_crop(batch, [32, 32], 4)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3UwYo-uu_MGb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Main Class\n",
        "\n",
        "class TripleGAN(object) :\n",
        "    def __init__(self, sess, epoch, batch_size, unlabel_batch_size, z_dim, dataset_name, n, gan_lr, cla_lr, result_dir):\n",
        "        self.sess = sess\n",
        "        self.dataset_name = dataset_name\n",
        "        #self.checkpoint_dir = checkpoint_dir\n",
        "        self.result_dir = result_dir\n",
        "        #self.log_dir = log_dir\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.unlabelled_batch_size = unlabel_batch_size\n",
        "        self.test_batch_size = 1000\n",
        "        self.model_name = \"TripleGAN\"     # name for checkpoint\n",
        "        if self.dataset_name == 'cifar10' :\n",
        "            self.input_height = 32\n",
        "            self.input_width = 32\n",
        "            self.output_height = 32\n",
        "            self.output_width = 32\n",
        "\n",
        "            self.z_dim = z_dim\n",
        "            self.y_dim = 10\n",
        "            self.c_dim = 3\n",
        "\n",
        "            self.learning_rate = gan_lr # 3e-4, 1e-3\n",
        "            self.cla_learning_rate = cla_lr # 3e-3, 1e-2 ?\n",
        "            self.GAN_beta1 = 0.5\n",
        "            self.beta1 = 0.9\n",
        "            self.beta2 = 0.999\n",
        "            self.epsilon = 1e-8\n",
        "            self.alpha = 0.5\n",
        "            self.alpha_cla_adv = 0.01\n",
        "            self.init_alpha_p = 0.0 # 0.1, 0.03\n",
        "            self.apply_alpha_p = 0.1\n",
        "            self.apply_epoch = 200 # 200, 300\n",
        "            self.decay_epoch = 50\n",
        "\n",
        "            self.sample_num = 64\n",
        "            self.visual_num = 100\n",
        "            self.len_discrete_code = 10\n",
        "\n",
        "            self.data_X, self.data_y, self.unlabelled_X, self.unlabelled_y, self.test_X, self.test_y = prepare_data(n) # trainX, trainY, testX, testY\n",
        "\n",
        "            self.num_batches = len(self.data_X) // self.batch_size\n",
        "\n",
        "        else :\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def discriminator(self, x, y_, scope='discriminator', is_training=True, reuse=False):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "            x = dropout(x, rate=0.2, is_training=is_training)\n",
        "            y = tf.reshape(y_, [-1, 1, 1, self.y_dim])\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=32, kernel=[3,3], layer_name=scope+'_conv1'))\n",
        "            x = conv_concat(x,y)\n",
        "            x = lrelu(conv_layer(x, filter_size=32, kernel=[3,3], stride=2, layer_name=scope+'_conv2'))\n",
        "            x = dropout(x, rate=0.2, is_training=is_training)\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=64, kernel=[3,3], layer_name=scope+'_conv3'))\n",
        "            x = conv_concat(x,y)\n",
        "            x = lrelu(conv_layer(x, filter_size=64, kernel=[3,3], stride=2, layer_name=scope+'_conv4'))\n",
        "            x = dropout(x, rate=0.2, is_training=is_training)\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv5'))\n",
        "            x = conv_concat(x,y)\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv6'))\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = Global_Average_Pooling(x)\n",
        "            x = flatten(x)\n",
        "            x = concat([x,y_]) # mlp_concat\n",
        "\n",
        "            x_logit = linear(x, unit=1, layer_name=scope+'_linear1')\n",
        "            out = sigmoid(x_logit)\n",
        "\n",
        "\n",
        "            return out, x_logit, x\n",
        "\n",
        "    def generator(self, z, y, scope='generator', is_training=True, reuse=False):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "\n",
        "            x = concat([z, y]) # mlp_concat\n",
        "\n",
        "            x = relu(linear(x, unit=512*4*4, layer_name=scope+'_linear1'))\n",
        "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch1')\n",
        "\n",
        "            x = tf.reshape(x, shape=[-1, 4, 4, 512])\n",
        "            y = tf.reshape(y, [-1, 1, 1, self.y_dim])\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = relu(deconv_layer(x, filter_size=256, kernel=[5,5], stride=2, layer_name=scope+'_deconv1'))\n",
        "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch2')\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = relu(deconv_layer(x, filter_size=128, kernel=[5,5], stride=2, layer_name=scope+'_deconv2'))\n",
        "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch3')\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = tanh(deconv_layer(x, filter_size=3, kernel=[5,5], stride=2, wn=False, layer_name=scope+'deconv3'))\n",
        "\n",
        "            return x\n",
        "    def classifier(self, x, scope='classifier', is_training=True, reuse=False):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "            x = gaussian_noise_layer(x) # default = 0.15\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv1'))\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv2'))\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv3'))\n",
        "\n",
        "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
        "            x = dropout(x, rate=0.5, is_training=is_training)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv4'))\n",
        "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv5'))\n",
        "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv6'))\n",
        "\n",
        "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
        "            x = dropout(x, rate=0.5, is_training=is_training)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=512, kernel=[3,3], layer_name=scope+'_conv7'))\n",
        "            x = nin(x, unit=256, layer_name=scope+'_nin1')\n",
        "            x = nin(x, unit=128, layer_name=scope+'_nin2')\n",
        "\n",
        "            x = Global_Average_Pooling(x)\n",
        "            x = flatten(x)\n",
        "            x = linear(x, unit=10, layer_name=scope+'_linear1')\n",
        "            return x\n",
        "\n",
        "    def build_model(self):\n",
        "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
        "        bs = self.batch_size\n",
        "        unlabel_bs = self.unlabelled_batch_size\n",
        "        test_bs = self.test_batch_size\n",
        "        alpha = self.alpha\n",
        "        alpha_cla_adv = self.alpha_cla_adv\n",
        "        self.alpha_p = tf.placeholder(tf.float32, name='alpha_p')\n",
        "        self.gan_lr = tf.placeholder(tf.float32, name='gan_lr')\n",
        "        self.cla_lr = tf.placeholder(tf.float32, name='cla_lr')\n",
        "        self.unsup_weight = tf.placeholder(tf.float32, name='unsup_weight')\n",
        "        self.c_beta1 = tf.placeholder(tf.float32, name='c_beta1')\n",
        "\n",
        "        \"\"\" Graph Input \"\"\"\n",
        "        # images\n",
        "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
        "        self.unlabelled_inputs = tf.placeholder(tf.float32, [unlabel_bs] + image_dims, name='unlabelled_images')\n",
        "        self.test_inputs = tf.placeholder(tf.float32, [test_bs] + image_dims, name='test_images')\n",
        "\n",
        "        # labels\n",
        "        self.y = tf.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
        "        self.unlabelled_inputs_y = tf.placeholder(tf.float32, [unlabel_bs, self.y_dim])\n",
        "        self.test_label = tf.placeholder(tf.float32, [test_bs, self.y_dim], name='test_label')\n",
        "        self.visual_y = tf.placeholder(tf.float32, [self.visual_num, self.y_dim], name='visual_y')\n",
        "\n",
        "        # noises\n",
        "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
        "        self.visual_z = tf.placeholder(tf.float32, [self.visual_num, self.z_dim], name='visual_z')\n",
        "\n",
        "        \"\"\" Loss Function \"\"\"\n",
        "        # A Game with Three Players\n",
        "\n",
        "        # output of D for real images\n",
        "        D_real, D_real_logits, _ = self.discriminator(self.inputs, self.y, is_training=True, reuse=False)\n",
        "\n",
        "        # output of D for fake images\n",
        "        G = self.generator(self.z, self.y, is_training=True, reuse=False)\n",
        "        D_fake, D_fake_logits, _ = self.discriminator(G, self.y, is_training=True, reuse=True)\n",
        "\n",
        "        # output of C for real images\n",
        "        C_real_logits = self.classifier(self.inputs, is_training=True, reuse=False)\n",
        "        R_L = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_real_logits))\n",
        "\n",
        "        # output of D for unlabelled images\n",
        "        Y_c = self.classifier(self.unlabelled_inputs, is_training=True, reuse=True)\n",
        "        D_cla, D_cla_logits, _ = self.discriminator(self.unlabelled_inputs, Y_c, is_training=True, reuse=True)\n",
        "\n",
        "        # output of C for fake images\n",
        "        C_fake_logits = self.classifier(G, is_training=True, reuse=True)\n",
        "        R_P = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_fake_logits))\n",
        "\n",
        "        #\n",
        "\n",
        "        # get loss for discriminator\n",
        "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
        "        d_loss_fake = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
        "        d_loss_cla = alpha*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.zeros_like(D_cla)))\n",
        "        self.d_loss = d_loss_real + d_loss_fake + d_loss_cla\n",
        "\n",
        "        # get loss for generator\n",
        "        self.g_loss = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
        "\n",
        "        # test loss for classify\n",
        "        test_Y = self.classifier(self.test_inputs, is_training=False, reuse=True)\n",
        "        correct_prediction = tf.equal(tf.argmax(test_Y, 1), tf.argmax(self.test_label, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        # get loss for classify\n",
        "        max_c = tf.cast(tf.argmax(Y_c, axis=1), tf.float32)\n",
        "        c_loss_dis = tf.reduce_mean(max_c * tf.nn.softmax_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.ones_like(D_cla)))\n",
        "        # self.c_loss = alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
        "\n",
        "        # R_UL = self.unsup_weight * tf.reduce_mean(tf.squared_difference(Y_c, self.unlabelled_inputs_y))\n",
        "        self.c_loss = alpha_cla_adv * alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
        "\n",
        "        \"\"\" Training \"\"\"\n",
        "\n",
        "        # divide trainable variables into a group for D and a group for G\n",
        "        t_vars = tf.trainable_variables()\n",
        "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "        c_vars = [var for var in t_vars if 'classifier' in var.name]\n",
        "\n",
        "        for var in t_vars: print(var.name)\n",
        "        # optimizers\n",
        "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "            self.d_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.d_loss, var_list=d_vars)\n",
        "            self.g_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.g_loss, var_list=g_vars)\n",
        "            self.c_optim = tf.train.AdamOptimizer(self.cla_lr, beta1=self.beta1, beta2=self.beta2, epsilon=self.epsilon).minimize(self.c_loss, var_list=c_vars)\n",
        "\n",
        "        \"\"\"\" Testing \"\"\"\n",
        "        # for test\n",
        "        self.fake_images = self.generator(self.visual_z, self.visual_y, is_training=False, reuse=True)\n",
        "\n",
        "        \"\"\" Summary \"\"\"\n",
        "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
        "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
        "        d_loss_cla_sum = tf.summary.scalar(\"d_loss_cla\", d_loss_cla)\n",
        "\n",
        "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
        "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
        "        c_loss_sum = tf.summary.scalar(\"c_loss\", self.c_loss)\n",
        "\n",
        "\n",
        "\n",
        "        # final summary operations\n",
        "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
        "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
        "        self.c_sum = tf.summary.merge([d_loss_cla_sum, c_loss_sum])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "        gan_lr = self.learning_rate\n",
        "        cla_lr = self.cla_learning_rate\n",
        "\n",
        "        # graph inputs for visualize training results\n",
        "        self.sample_z = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
        "        self.test_codes = self.data_y[0:self.visual_num]\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # summary writer\n",
        "        #self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        #could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        '''if could_load:\n",
        "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
        "            counter = checkpoint_counter\n",
        "            with open('lr_logs.txt', 'r') as f :\n",
        "                line = f.readlines()\n",
        "                line = line[-1]\n",
        "                gan_lr = float(line.split()[0])\n",
        "                cla_lr = float(line.split()[1])\n",
        "                print(\"gan_lr : \", gan_lr)\n",
        "                print(\"cla_lr : \", cla_lr)\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")'''\n",
        "\n",
        "        # loop for epoch\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.epoch):\n",
        "\n",
        "            if epoch >= self.decay_epoch :\n",
        "                gan_lr *= 0.995\n",
        "                cla_lr *= 0.99\n",
        "                print(\"**** learning rate DECAY ****\")\n",
        "                print(gan_lr)\n",
        "                print(cla_lr)\n",
        "\n",
        "            if epoch >= self.apply_epoch :\n",
        "                alpha_p = self.apply_alpha_p\n",
        "            else :\n",
        "                alpha_p = self.init_alpha_p\n",
        "\n",
        "            rampup_value = rampup(epoch - 1)\n",
        "            unsup_weight = rampup_value * 100.0 if epoch > 1 else 0\n",
        "\n",
        "            # get batch data\n",
        "            for idx in range(self.num_batches):\n",
        "                batch_images = self.data_X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "                batch_codes = self.data_y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "                batch_unlabelled_images = self.unlabelled_X[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
        "                batch_unlabelled_images_y = self.unlabelled_y[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
        "\n",
        "                batch_z = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
        "\n",
        "                feed_dict = {\n",
        "                    self.inputs: batch_images, self.y: batch_codes,\n",
        "                    self.unlabelled_inputs: batch_unlabelled_images,\n",
        "                    self.unlabelled_inputs_y: batch_unlabelled_images_y,\n",
        "                    self.z: batch_z, self.alpha_p: alpha_p,\n",
        "                    self.gan_lr: gan_lr, self.cla_lr: cla_lr,\n",
        "                    self.unsup_weight : unsup_weight\n",
        "                }\n",
        "                # update D network\n",
        "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss], feed_dict=feed_dict)\n",
        "                #self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # update G network\n",
        "                _, summary_str_g, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], feed_dict=feed_dict)\n",
        "                #self.writer.add_summary(summary_str_g, counter)\n",
        "\n",
        "                # update C network\n",
        "                _, summary_str_c, c_loss = self.sess.run([self.c_optim, self.c_sum, self.c_loss], feed_dict=feed_dict)\n",
        "                #self.writer.add_summary(summary_str_c, counter)\n",
        "\n",
        "                # display training status\n",
        "                #counter += 1\n",
        "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, c_loss: %.8f\" \\\n",
        "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss, c_loss))\n",
        "\n",
        "                # save training results for every 100 steps\n",
        "\n",
        "\n",
        "            # classifier test\n",
        "            test_acc = 0.0\n",
        "\n",
        "            for idx in range(10) :\n",
        "                test_batch_x = self.test_X[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
        "                test_batch_y = self.test_y[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
        "\n",
        "                acc_ = self.sess.run(self.accuracy, feed_dict={\n",
        "                    self.test_inputs: test_batch_x,\n",
        "                    self.test_label: test_batch_y\n",
        "                })\n",
        "\n",
        "                test_acc += acc_\n",
        "            test_acc /= 10\n",
        "\n",
        "            summary_test = tf.Summary(value=[tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
        "            #self.writer.add_summary(summary_test, epoch)\n",
        "\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model\n",
        "            #self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "            # show temporal results\n",
        "            #self.visualize_results(epoch)\n",
        "\n",
        "            # save model for final step\n",
        "        #self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    def visualize_results(self, epoch):\n",
        "        # tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "        image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
        "        z_sample = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
        "\n",
        "        \"\"\" random noise, random discrete code, fixed continuous code \"\"\"\n",
        "        y = np.random.choice(self.len_discrete_code, self.visual_num)\n",
        "        # Generated 10 labels with batch_size\n",
        "        y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
        "        y_one_hot[np.arange(self.visual_num), y] = 1\n",
        "        \n",
        "        \n",
        "        \"\"\" specified condition, random noise \"\"\"\n",
        "\n",
        "        np.random.seed()\n",
        "        si = np.random.choice(self.visual_num, n_styles)\n",
        "\n",
        "        for l in range(self.len_discrete_code):\n",
        "            y = np.zeros(self.visual_num, dtype=np.int64) + l\n",
        "            y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
        "            y_one_hot[np.arange(self.visual_num), y] = 1\n",
        "\n",
        "            samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
        "            save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
        "                            self.result_dir + '/' + self.model_name + '_epoch%03d' % epoch + '_test_class_%d.png' % l)\n",
        "\n",
        "            samples = samples[si, :, :, :]\n",
        "\n",
        "            if l == 0:\n",
        "                all_samples = samples\n",
        "            else:\n",
        "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
        "\n",
        "        \"\"\" save merged images to check style-consistency \"\"\"\n",
        "        canvas = np.zeros_like(all_samples)\n",
        "        for s in range(n_styles):\n",
        "            for c in range(self.len_discrete_code):\n",
        "                canvas[s * self.len_discrete_code + c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
        "\n",
        "        save_images(canvas, [n_styles, self.len_discrete_code],\n",
        "                        self.result_dir + '/'+  self.model_name + '_epoch%03d' % epoch + '_test_all_classes_style_by_style.png')\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return \"{}_{}_{}_{}\".format(\n",
        "            self.model_name, self.dataset_name,\n",
        "            self.batch_size, self.z_dim)\n",
        "\n",
        "\n",
        "       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wceErOo_hzJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": []
        },
        "outputId": "4610d35e-4728-4ce6-ed71-8ef15ac046b1",
        "extraOutputs": [
          {
            "data": [],
            "output_extras": [],
            "id": "5e4bfcba-61c6-4b3b-9e75-09332086e0cb"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "a4c506bb-f4dc-4e1b-fce1-852678844c9b"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "99788c25-63f7-4726-d0c0-740740a552c8"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "67ca3004-2d40-495b-8e36-12ca3a5feed8"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "6a3ef638-3a13-4dd2-acee-5b8deb48f773"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "24ea561e-9662-4a2d-c766-0c71d4e12170"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "6a5b3925-044c-499d-ab9a-ee5e17842f18"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "79d8a100-4f0b-4b7e-d03f-304d90b03ba0"
          },
          {
            "data": [],
            "output_extras": [],
            "id": "9fe6bb8f-e439-463b-f75b-8dd47f44fb35"
          },
          {
            "data": [
              {
                "output_type": "stream",
                "text": [
                  "('labelled data:', (4000, 32, 32, 3), (4000, 1))\n",
                  "('unlabelled data :', (50000, 32, 32, 3), (50000, 1))\n",
                  "('Test data :', (10000, 32, 32, 3), (10000, 1))\n",
                  "======Load finished======\n",
                  "======Shuffling data======\n",
                  "======Prepare Finished======\n",
                  "WARNING:tensorflow:From <ipython-input-9-07748f839822>:171: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
                  "Instructions for updating:\n",
                  "\n",
                  "Future major versions of TensorFlow will allow gradients to flow\n",
                  "into the labels input on backprop by default.\n",
                  "\n",
                  "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
                  "\n",
                  "discriminator/conv2d/kernel:0\n",
                  "discriminator/conv2d/bias:0\n",
                  "discriminator/conv2d_1/kernel:0\n",
                  "discriminator/conv2d_1/bias:0\n",
                  "discriminator/conv2d_2/kernel:0\n",
                  "discriminator/conv2d_2/bias:0\n",
                  "discriminator/conv2d_3/kernel:0\n",
                  "discriminator/conv2d_3/bias:0\n",
                  "discriminator/conv2d_4/kernel:0\n",
                  "discriminator/conv2d_4/bias:0\n",
                  "discriminator/conv2d_5/kernel:0\n",
                  "discriminator/conv2d_5/bias:0\n",
                  "discriminator/dense/kernel:0\n",
                  "discriminator/dense/bias:0\n",
                  "generator/dense/kernel:0\n",
                  "generator/dense/bias:0\n",
                  "generator/generator_batch1/beta:0\n",
                  "generator/generator_batch1/gamma:0\n",
                  "generator/conv2d_transpose/kernel:0\n",
                  "generator/conv2d_transpose/bias:0\n",
                  "generator/generator_batch2/beta:0\n",
                  "generator/generator_batch2/gamma:0\n",
                  "generator/conv2d_transpose_1/kernel:0\n",
                  "generator/conv2d_transpose_1/bias:0\n",
                  "generator/generator_batch3/beta:0\n",
                  "generator/generator_batch3/gamma:0\n",
                  "generator/conv2d_transpose_2/kernel:0\n",
                  "generator/conv2d_transpose_2/bias:0\n",
                  "classifier/conv2d/kernel:0\n",
                  "classifier/conv2d/bias:0\n",
                  "classifier/conv2d_1/kernel:0\n",
                  "classifier/conv2d_1/bias:0\n",
                  "classifier/conv2d_2/kernel:0\n",
                  "classifier/conv2d_2/bias:0\n",
                  "classifier/conv2d_3/kernel:0\n",
                  "classifier/conv2d_3/bias:0\n",
                  "classifier/conv2d_4/kernel:0\n",
                  "classifier/conv2d_4/bias:0\n",
                  "classifier/conv2d_5/kernel:0\n",
                  "classifier/conv2d_5/bias:0\n",
                  "classifier/conv2d_6/kernel:0\n",
                  "classifier/conv2d_6/bias:0\n",
                  "classifier/dense/kernel:0\n",
                  "classifier/dense/bias:0\n",
                  "classifier/dense_1/kernel:0\n",
                  "classifier/dense_1/bias:0\n",
                  "classifier/dense_2/kernel:0\n",
                  "classifier/dense_2/bias:0\n",
                  "Epoch: [ 0] [   0/ 200] time: 6.4496, d_loss: 2.47887135, g_loss: 0.37709081, c_loss: 6.78079987\n",
                  "Epoch: [ 0] [   1/ 200] time: 7.3141, d_loss: 30.79489517, g_loss: 0.46551901, c_loss: 125.13276672\n",
                  "Epoch: [ 0] [   2/ 200] time: 8.1765, d_loss: 1.17238414, g_loss: 0.50674784, c_loss: 13.09961891\n",
                  "Epoch: [ 0] [   3/ 200] time: 9.0369, d_loss: 1.17263067, g_loss: 0.53708553, c_loss: 47.87664032\n",
                  "Epoch: [ 0] [   4/ 200] time: 9.8987, d_loss: 1.20054555, g_loss: 0.55397737, c_loss: 8.38925743\n",
                  "Epoch: [ 0] [   5/ 200] time: 10.7627, d_loss: 1.16066813, g_loss: 0.51520002, c_loss: 47.32758713\n",
                  "Epoch: [ 0] [   6/ 200] time: 11.6212, d_loss: 1.19313920, g_loss: 0.54469651, c_loss: 21.85113907\n",
                  "Epoch: [ 0] [   7/ 200] time: 12.4858, d_loss: 1.19086015, g_loss: 0.52263355, c_loss: 57.73201370\n",
                  "Epoch: [ 0] [   8/ 200] time: 13.3452, d_loss: 5.38812447, g_loss: 0.61581075, c_loss: 36.15936279\n",
                  "Epoch: [ 0] [   9/ 200] time: 14.2097, d_loss: 1.25876081, g_loss: 0.67618972, c_loss: 11.69824028\n",
                  "Epoch: [ 0] [  10/ 200] time: 15.0657, d_loss: 1.34361362, g_loss: 0.68980354, c_loss: 39.83747482\n",
                  "Epoch: [ 0] [  11/ 200] time: 15.9436, d_loss: 1.33532584, g_loss: 0.70326900, c_loss: 37.40036392\n",
                  "Epoch: [ 0] [  12/ 200] time: 16.8044, d_loss: 1.38511002, g_loss: 0.71427786, c_loss: 41.84482956\n",
                  "Epoch: [ 0] [  13/ 200] time: 17.6663, d_loss: 1.36210811, g_loss: 0.69598520, c_loss: 22.43837929\n",
                  "Epoch: [ 0] [  14/ 200] time: 18.5294, d_loss: 1.33816218, g_loss: 0.68108332, c_loss: 18.02204514\n",
                  "Epoch: [ 0] [  15/ 200] time: 19.3943, d_loss: 1.32846701, g_loss: 0.65972114, c_loss: 14.24591827\n",
                  "Epoch: [ 0] [  16/ 200] time: 20.2580, d_loss: 1.31622660, g_loss: 0.66400504, c_loss: 10.88573170\n",
                  "Epoch: [ 0] [  17/ 200] time: 21.1252, d_loss: 1.27385581, g_loss: 0.63118517, c_loss: 7.40263653\n",
                  "Epoch: [ 0] [  18/ 200] time: 22.0023, d_loss: 1.30568647, g_loss: 0.64068270, c_loss: 18.72781372\n",
                  "Epoch: [ 0] [  19/ 200] time: 22.8724, d_loss: 1.33421671, g_loss: 0.63167578, c_loss: 14.42075920\n",
                  "Epoch: [ 0] [  20/ 200] time: 23.7320, d_loss: 1.28444135, g_loss: 0.63169175, c_loss: 7.10081768\n"
                ],
                "name": "stdout"
              }
            ],
            "output_extras": [
              {
                "item_id": 6
              }
            ],
            "id": "f40366ea-f361-4eb7-bc66-fadd2950632a",
            "base_uri": "https://localhost:8080/",
            "height": 1448
          }
        ]
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"main\"\"\"\n",
        "def main():\n",
        "    # parse arguments\n",
        "    '''args = parser.parse_args()\n",
        "    if args is None:\n",
        "      exit()'''\n",
        "\n",
        "    # open session\n",
        "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        gan = TripleGAN(sess, epoch=5, batch_size=20, unlabel_batch_size=250,\n",
        "                        z_dim=100, dataset_name='cifar10', n=4000, gan_lr = 2e-4, cla_lr = 2e-3,\n",
        "                         result_dir='drive')\n",
        "\n",
        "        # build graph\n",
        "        gan.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        #show_all_variables()\n",
        "\n",
        "        # launch the graph in a session\n",
        "        gan.train()\n",
        "        print(\" [*] Training finished!\")\n",
        "\n",
        "        # visualize learned generator\n",
        "        gan.visualize_results(4)\n",
        "        print(\" [*] Testing finished!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}