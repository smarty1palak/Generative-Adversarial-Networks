{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "from ops import *\n",
    "from utils import *\n",
    "import time\n",
    "\n",
    "from utils import show_all_variables\n",
    "from utils import check_folder\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleGAN(object) :\n",
    "    def __init__(self, sess, epoch, batch_size, unlabel_batch_size, z_dim, dataset_name, n, gan_lr, cla_lr, checkpoint_dir, result_dir, log_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.unlabelled_batch_size = unlabel_batch_size\n",
    "        self.test_batch_size = 1000\n",
    "        self.model_name = \"TripleGAN\"     # name for checkpoint\n",
    "        if self.dataset_name == 'cifar10' :\n",
    "            self.input_height = 32\n",
    "            self.input_width = 32\n",
    "            self.output_height = 32\n",
    "            self.output_width = 32\n",
    "\n",
    "            self.z_dim = z_dim\n",
    "            self.y_dim = 10\n",
    "            self.c_dim = 3\n",
    "\n",
    "            self.learning_rate = gan_lr # 3e-4, 1e-3\n",
    "            self.cla_learning_rate = cla_lr # 3e-3, 1e-2 ?\n",
    "            self.GAN_beta1 = 0.5\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.epsilon = 1e-8\n",
    "            self.alpha = 0.5\n",
    "            self.alpha_cla_adv = 0.01\n",
    "            self.init_alpha_p = 0.0 # 0.1, 0.03\n",
    "            self.apply_alpha_p = 0.1\n",
    "            self.apply_epoch = 200 # 200, 300\n",
    "            self.decay_epoch = 50\n",
    "\n",
    "            self.sample_num = 64\n",
    "            self.visual_num = 100\n",
    "            self.len_discrete_code = 10\n",
    "\n",
    "            self.data_X, self.data_y, self.unlabelled_X, self.unlabelled_y, self.test_X, self.test_y = cifar10.prepare_data(n) # trainX, trainY, testX, testY\n",
    "\n",
    "            self.num_batches = len(self.data_X) // self.batch_size\n",
    "\n",
    "        else :\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def discriminator(self, x, y_, scope='discriminator', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "            x = dropout(x, rate=0.2, is_training=is_training)\n",
    "            y = tf.reshape(y_, [-1, 1, 1, self.y_dim])\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=32, kernel=[3,3], layer_name=scope+'_conv1'))\n",
    "            x = conv_concat(x,y)\n",
    "            x = lrelu(conv_layer(x, filter_size=32, kernel=[3,3], stride=2, layer_name=scope+'_conv2'))\n",
    "            x = dropout(x, rate=0.2, is_training=is_training)\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=64, kernel=[3,3], layer_name=scope+'_conv3'))\n",
    "            x = conv_concat(x,y)\n",
    "            x = lrelu(conv_layer(x, filter_size=64, kernel=[3,3], stride=2, layer_name=scope+'_conv4'))\n",
    "            x = dropout(x, rate=0.2, is_training=is_training)\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv5'))\n",
    "            x = conv_concat(x,y)\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv6'))\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = Global_Average_Pooling(x)\n",
    "            x = flatten(x)\n",
    "            x = concat([x,y_]) # mlp_concat\n",
    "\n",
    "            x_logit = linear(x, unit=1, layer_name=scope+'_linear1')\n",
    "            out = sigmoid(x_logit)\n",
    "\n",
    "\n",
    "            return out, x_logit, x\n",
    "\n",
    "    def generator(self, z, y, scope='generator', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "\n",
    "            x = concat([z, y]) # mlp_concat\n",
    "\n",
    "            x = relu(linear(x, unit=512*4*4, layer_name=scope+'_linear1'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch1')\n",
    "\n",
    "            x = tf.reshape(x, shape=[-1, 4, 4, 512])\n",
    "            y = tf.reshape(y, [-1, 1, 1, self.y_dim])\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = relu(deconv_layer(x, filter_size=256, kernel=[5,5], stride=2, layer_name=scope+'_deconv1'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch2')\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = relu(deconv_layer(x, filter_size=128, kernel=[5,5], stride=2, layer_name=scope+'_deconv2'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch3')\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = tanh(deconv_layer(x, filter_size=3, kernel=[5,5], stride=2, wn=False, layer_name=scope+'deconv3'))\n",
    "\n",
    "            return x\n",
    "    def classifier(self, x, scope='classifier', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "            x = gaussian_noise_layer(x) # default = 0.15\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv1'))\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv2'))\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv3'))\n",
    "\n",
    "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
    "            x = dropout(x, rate=0.5, is_training=is_training)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv4'))\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv5'))\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv6'))\n",
    "\n",
    "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
    "            x = dropout(x, rate=0.5, is_training=is_training)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=512, kernel=[3,3], layer_name=scope+'_conv7'))\n",
    "            x = nin(x, unit=256, layer_name=scope+'_nin1')\n",
    "            x = nin(x, unit=128, layer_name=scope+'_nin2')\n",
    "\n",
    "            x = Global_Average_Pooling(x)\n",
    "            x = flatten(x)\n",
    "            x = linear(x, unit=10, layer_name=scope+'_linear1')\n",
    "            return x\n",
    "\n",
    "    def build_model(self):\n",
    "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "        bs = self.batch_size\n",
    "        unlabel_bs = self.unlabelled_batch_size\n",
    "        test_bs = self.test_batch_size\n",
    "        alpha = self.alpha\n",
    "        alpha_cla_adv = self.alpha_cla_adv\n",
    "        self.alpha_p = tf.placeholder(tf.float32, name='alpha_p')\n",
    "        self.gan_lr = tf.placeholder(tf.float32, name='gan_lr')\n",
    "        self.cla_lr = tf.placeholder(tf.float32, name='cla_lr')\n",
    "        self.unsup_weight = tf.placeholder(tf.float32, name='unsup_weight')\n",
    "        self.c_beta1 = tf.placeholder(tf.float32, name='c_beta1')\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
    "        self.unlabelled_inputs = tf.placeholder(tf.float32, [unlabel_bs] + image_dims, name='unlabelled_images')\n",
    "        self.test_inputs = tf.placeholder(tf.float32, [test_bs] + image_dims, name='test_images')\n",
    "\n",
    "        # labels\n",
    "        self.y = tf.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
    "        self.unlabelled_inputs_y = tf.placeholder(tf.float32, [unlabel_bs, self.y_dim])\n",
    "        self.test_label = tf.placeholder(tf.float32, [test_bs, self.y_dim], name='test_label')\n",
    "        self.visual_y = tf.placeholder(tf.float32, [self.visual_num, self.y_dim], name='visual_y')\n",
    "\n",
    "        # noises\n",
    "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
    "        self.visual_z = tf.placeholder(tf.float32, [self.visual_num, self.z_dim], name='visual_z')\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "        # A Game with Three Players\n",
    "\n",
    "        # output of D for real images\n",
    "        D_real, D_real_logits, _ = self.discriminator(self.inputs, self.y, is_training=True, reuse=False)\n",
    "\n",
    "        # output of D for fake images\n",
    "        G = self.generator(self.z, self.y, is_training=True, reuse=False)\n",
    "        D_fake, D_fake_logits, _ = self.discriminator(G, self.y, is_training=True, reuse=True)\n",
    "\n",
    "        # output of C for real images\n",
    "        C_real_logits = self.classifier(self.inputs, is_training=True, reuse=False)\n",
    "        R_L = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_real_logits))\n",
    "\n",
    "        # output of D for unlabelled images\n",
    "        Y_c = self.classifier(self.unlabelled_inputs, is_training=True, reuse=True)\n",
    "        D_cla, D_cla_logits, _ = self.discriminator(self.unlabelled_inputs, Y_c, is_training=True, reuse=True)\n",
    "\n",
    "        # output of C for fake images\n",
    "        C_fake_logits = self.classifier(G, is_training=True, reuse=True)\n",
    "        R_P = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_fake_logits))\n",
    "\n",
    "        #\n",
    "\n",
    "        # get loss for discriminator\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "        d_loss_fake = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "        d_loss_cla = alpha*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.zeros_like(D_cla)))\n",
    "        self.d_loss = d_loss_real + d_loss_fake + d_loss_cla\n",
    "\n",
    "        # get loss for generator\n",
    "        self.g_loss = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "        # test loss for classify\n",
    "        test_Y = self.classifier(self.test_inputs, is_training=False, reuse=True)\n",
    "        correct_prediction = tf.equal(tf.argmax(test_Y, 1), tf.argmax(self.test_label, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # get loss for classify\n",
    "        max_c = tf.cast(tf.argmax(Y_c, axis=1), tf.float32)\n",
    "        c_loss_dis = tf.reduce_mean(max_c * tf.nn.softmax_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.ones_like(D_cla)))\n",
    "        # self.c_loss = alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
    "\n",
    "        # R_UL = self.unsup_weight * tf.reduce_mean(tf.squared_difference(Y_c, self.unlabelled_inputs_y))\n",
    "        self.c_loss = alpha_cla_adv * alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
    "\n",
    "        \"\"\" Training \"\"\"\n",
    "\n",
    "        # divide trainable variables into a group for D and a group for G\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        c_vars = [var for var in t_vars if 'classifier' in var.name]\n",
    "\n",
    "        for var in t_vars: print(var.name)\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.d_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.d_loss, var_list=d_vars)\n",
    "            self.g_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.g_loss, var_list=g_vars)\n",
    "            self.c_optim = tf.train.AdamOptimizer(self.cla_lr, beta1=self.beta1, beta2=self.beta2, epsilon=self.epsilon).minimize(self.c_loss, var_list=c_vars)\n",
    "\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        self.fake_images = self.generator(self.visual_z, self.visual_y, is_training=False, reuse=True)\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "        d_loss_cla_sum = tf.summary.scalar(\"d_loss_cla\", d_loss_cla)\n",
    "\n",
    "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        c_loss_sum = tf.summary.scalar(\"c_loss\", self.c_loss)\n",
    "\n",
    "\n",
    "\n",
    "        # final summary operations\n",
    "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
    "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
    "        self.c_sum = tf.summary.merge([d_loss_cla_sum, c_loss_sum])\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        gan_lr = self.learning_rate\n",
    "        cla_lr = self.cla_learning_rate\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.sample_z = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
    "        self.test_codes = self.data_y[0:self.visual_num]\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            counter = checkpoint_counter\n",
    "            with open('lr_logs.txt', 'r') as f :\n",
    "                line = f.readlines()\n",
    "                line = line[-1]\n",
    "                gan_lr = float(line.split()[0])\n",
    "                cla_lr = float(line.split()[1])\n",
    "                print(\"gan_lr : \", gan_lr)\n",
    "                print(\"cla_lr : \", cla_lr)\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "\n",
    "            if epoch >= self.decay_epoch :\n",
    "                gan_lr *= 0.995\n",
    "                cla_lr *= 0.99\n",
    "                print(\"**** learning rate DECAY ****\")\n",
    "                print(gan_lr)\n",
    "                print(cla_lr)\n",
    "\n",
    "            if epoch >= self.apply_epoch :\n",
    "                alpha_p = self.apply_alpha_p\n",
    "            else :\n",
    "                alpha_p = self.init_alpha_p\n",
    "\n",
    "            rampup_value = rampup(epoch - 1)\n",
    "            unsup_weight = rampup_value * 100.0 if epoch > 1 else 0\n",
    "\n",
    "            # get batch data\n",
    "            for idx in range(start_batch_id, self.num_batches):\n",
    "                batch_images = self.data_X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "                batch_codes = self.data_y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "                batch_unlabelled_images = self.unlabelled_X[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
    "                batch_unlabelled_images_y = self.unlabelled_y[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
    "\n",
    "                batch_z = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.inputs: batch_images, self.y: batch_codes,\n",
    "                    self.unlabelled_inputs: batch_unlabelled_images,\n",
    "                    self.unlabelled_inputs_y: batch_unlabelled_images_y,\n",
    "                    self.z: batch_z, self.alpha_p: alpha_p,\n",
    "                    self.gan_lr: gan_lr, self.cla_lr: cla_lr,\n",
    "                    self.unsup_weight : unsup_weight\n",
    "                }\n",
    "                # update D network\n",
    "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # update G network\n",
    "                _, summary_str_g, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary_str_g, counter)\n",
    "\n",
    "                # update C network\n",
    "                _, summary_str_c, c_loss = self.sess.run([self.c_optim, self.c_sum, self.c_loss], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary_str_c, counter)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, c_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss, c_loss))\n",
    "\n",
    "                # save training results for every 100 steps\n",
    "                \"\"\"\n",
    "                if np.mod(counter, 100) == 0:\n",
    "                    samples = self.sess.run(self.fake_images,\n",
    "                                            feed_dict={self.z: self.sample_z, self.y: self.test_codes})\n",
    "                    image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
    "                    save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                                './' + check_folder(\n",
    "                                    self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_train_{:02d}_{:04d}.png'.format(\n",
    "                                    epoch, idx))\n",
    "                \"\"\"\n",
    "\n",
    "            # classifier test\n",
    "            test_acc = 0.0\n",
    "\n",
    "            for idx in range(10) :\n",
    "                test_batch_x = self.test_X[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
    "                test_batch_y = self.test_y[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
    "\n",
    "                acc_ = self.sess.run(self.accuracy, feed_dict={\n",
    "                    self.test_inputs: test_batch_x,\n",
    "                    self.test_label: test_batch_y\n",
    "                })\n",
    "\n",
    "                test_acc += acc_\n",
    "            test_acc /= 10\n",
    "\n",
    "            summary_test = tf.Summary(value=[tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
    "            self.writer.add_summary(summary_test, epoch)\n",
    "\n",
    "            line = \"Epoch: [%2d], test_acc: %.4f\\n\" % (epoch, test_acc)\n",
    "            print(line)\n",
    "            lr = \"{} {}\".format(gan_lr, cla_lr)\n",
    "            with open('logs.txt', 'a') as f:\n",
    "                f.write(line)\n",
    "            with open('lr_logs.txt', 'a') as f :\n",
    "                f.write(lr+'\\n')\n",
    "\n",
    "            # After an epoch, start_batch_id is set to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "            # show temporal results\n",
    "            self.visualize_results(epoch)\n",
    "\n",
    "            # save model for final step\n",
    "        self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "    def visualize_results(self, epoch):\n",
    "        # tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
    "        z_sample = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
    "\n",
    "        \"\"\" random noise, random discrete code, fixed continuous code \"\"\"\n",
    "        y = np.random.choice(self.len_discrete_code, self.visual_num)\n",
    "        # Generated 10 labels with batch_size\n",
    "        y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
    "        y_one_hot[np.arange(self.visual_num), y] = 1\n",
    "\n",
    "        samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
    "\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                    check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir + '/all_classes') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
    "\n",
    "        \"\"\" specified condition, random noise \"\"\"\n",
    "        n_styles = 10  # must be less than or equal to self.batch_size\n",
    "\n",
    "        np.random.seed()\n",
    "        si = np.random.choice(self.visual_num, n_styles)\n",
    "\n",
    "        for l in range(self.len_discrete_code):\n",
    "            y = np.zeros(self.visual_num, dtype=np.int64) + l\n",
    "            y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
    "            y_one_hot[np.arange(self.visual_num), y] = 1\n",
    "\n",
    "            samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
    "            save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                        check_folder(\n",
    "                            self.result_dir + '/' + self.model_dir + '/class_%d' % l) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_class_%d.png' % l)\n",
    "\n",
    "            samples = samples[si, :, :, :]\n",
    "\n",
    "            if l == 0:\n",
    "                all_samples = samples\n",
    "            else:\n",
    "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
    "\n",
    "        \"\"\" save merged images to check style-consistency \"\"\"\n",
    "        canvas = np.zeros_like(all_samples)\n",
    "        for s in range(n_styles):\n",
    "            for c in range(self.len_discrete_code):\n",
    "                canvas[s * self.len_discrete_code + c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
    "\n",
    "        save_images(canvas, [n_styles, self.len_discrete_code],\n",
    "                    check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir + '/all_classes_style_by_style') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes_style_by_style.png')\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}_{}\".format(\n",
    "            self.model_name, self.dataset_name,\n",
    "            self.batch_size, self.z_dim)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_args() takes exactly 1 argument (11 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2206127fbf53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m      \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2206127fbf53>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# parse arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_args() takes exactly 1 argument (11 given)"
     ]
    }
   ],
   "source": [
    "\"\"\"parsing and configuration\"\"\"\n",
    "def parse_args(int ):\n",
    "    desc = \"Tensorflow implementation of TripleGAN\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "    parser.add_argument('--n', type=int, default=4000, help='The number of dataset')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10', choices=['mnist', 'fashion-mnist', 'celebA', 'cifar10'],\n",
    "                        help='The name of dataset')\n",
    "    # In now, only cifar 10...\n",
    "    parser.add_argument('--epoch', type=int, default=1000, help='The number of epochs to run')\n",
    "    parser.add_argument('--batch_size', type=int, default=20, help='The size of batch')\n",
    "    parser.add_argument('--unlabel_batch_size', type=int, default=250, help='The size of unlabel batch')\n",
    "    parser.add_argument('--z_dim', type=int, default=100, help='Dimension of noise vector')\n",
    "    parser.add_argument('--gan_lr', type=float, default=2e-4, help='learning rate of GAN')\n",
    "    parser.add_argument('--cla_lr', type=float, default=2e-3, help='learning rate of Classify')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n",
    "                        help='Directory name to save the checkpoints')\n",
    "    parser.add_argument('--result_dir', type=str, default='results',\n",
    "                        help='Directory name to save the generated images')\n",
    "    parser.add_argument('--log_dir', type=str, default='logs',\n",
    "                        help='Directory name to save training logs')\n",
    "\n",
    "    return check_args(parser.parse_args())\n",
    "\n",
    "\"\"\"checking arguments\"\"\"\n",
    "def check_args(args):\n",
    "    # --checkpoint_dir\n",
    "    check_folder(args.checkpoint_dir)\n",
    "\n",
    "    # --result_dir\n",
    "    check_folder(args.result_dir)\n",
    "\n",
    "    # --result_dir\n",
    "    check_folder(args.log_dir)\n",
    "\n",
    "    # --epoch\n",
    "    try:\n",
    "        assert args.epoch >= 1\n",
    "    except:\n",
    "        print('number of epochs must be larger than or equal to one')\n",
    "\n",
    "    # --batch_size\n",
    "    try:\n",
    "        assert args.batch_size >= 1\n",
    "    except:\n",
    "        print('batch size must be larger than or equal to one')\n",
    "\n",
    "    # --z_dim\n",
    "    try:\n",
    "        assert args.z_dim >= 1\n",
    "    except:\n",
    "        print('dimension of noise vector must be larger than or equal to one')\n",
    "\n",
    "    return args\n",
    "\n",
    "\"\"\"main\"\"\"\n",
    "def main():\n",
    "    # parse arguments\n",
    "    args = parse_args(4000,'cifar10',1000,20,250,100,2e-4,2e-3,'checkpoints','results','log')\n",
    "    if args is None:\n",
    "        exit()\n",
    "\n",
    "    # open session\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "        gan = TripleGAN(sess, epoch=args.epoch, batch_size=args.batch_size, unlabel_batch_size=args.unlabel_batch_size,\n",
    "                        z_dim=args.z_dim, dataset_name=args.dataset, n=args.n, gan_lr = args.gan_lr, cla_lr = args.cla_lr,\n",
    "                        checkpoint_dir=args.checkpoint_dir, result_dir=args.result_dir, log_dir=args.log_dir)\n",
    "\n",
    "        # build graph\n",
    "        gan.build_model()\n",
    "\n",
    "        # show network architecture\n",
    "        show_all_variables()\n",
    "\n",
    "        # launch the graph in a session\n",
    "        gan.train()\n",
    "        print(\" [*] Training finished!\")\n",
    "\n",
    "        # visualize learned generator\n",
    "        gan.visualize_results(args.epoch-1)\n",
    "        print(\" [*] Testing finished!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9f8535eb12fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m      \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9f8535eb12fd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# parse arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9f8535eb12fd>\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                         help='Directory name to save training logs')\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\"\"\"checking arguments\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/palak/anaconda2/envs/tensorflow/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/palak/anaconda2/envs/tensorflow/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         \"\"\"\n\u001b[1;32m   2373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: error: %s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/palak/anaconda2/envs/tensorflow/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
